# Import the 'pipeline' decorator from ZenML framework
# This allows us to define a machine learning pipeline structure
from zenml import pipeline

# Import specific ETL step functions from the local 'steps.etl' module:
# - crawl_links: A function to extract data from web URLs
# - get_or_create_user: A function to retrieve an existing user or create a new one
from steps.etl import crawl_links, get_or_create_user


# Apply the 'pipeline' decorator to define a ZenML pipeline named 'digital_data_etl'
# This transforms a regular function into a pipeline that ZenML can orchestrate and track
@pipeline
def digital_data_etl(user_full_name: str, links: list[str]) -> str:
    # First step: Get or create a user entity based on the provided full name
    # This likely connects to a database to find an existing user or register a new one
    user = get_or_create_user(user_full_name)
    
    # Second step: Process the provided list of URLs using the user entity
    # This step crawls each link, extracts relevant data, and associates it with the user
    # The user parameter enables tracking who initiated the data collection
    last_step = crawl_links(user=user, links=links)

    # Return the unique invocation ID generated by ZenML for the crawl_links step
    # This ID can be used later to reference this specific pipeline run or its artifacts
    return last_step.invocation_id
